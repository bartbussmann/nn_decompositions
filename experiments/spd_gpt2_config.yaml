# SPD decomposition of GPT-2 layer 8 MLP (c_fc + c_proj).
# Targets the same MLP as the transcoder experiments in main.py.

# --- WandB ---
wandb_project: "gpt2_spd"
wandb_run_name: null
wandb_run_name_prefix: ""

# --- General ---
seed: 0
n_mask_samples: 1
ci_fn_type: mlp
ci_fn_hidden_dims: [16]
sigmoid_type: leaky_hard
sampling: continuous
use_delta_component: true

# --- Modules to decompose ---
module_info:
  - module_pattern: "transformer.h.8.mlp.c_fc"     # Linear(768, 3072)
    C: 6144
  - module_pattern: "transformer.h.8.mlp.c_proj"    # Linear(3072, 768)
    C: 6144
identity_module_info: null

# --- Loss ---
loss_metric_configs:
  - classname: ImportanceMinimalityLoss
    coeff: 0.0001
    pnorm: 2.0
    beta: 0
    p_anneal_start_frac: 0.25
    p_anneal_final_p: 0.3
    p_anneal_end_frac: 0.75
  - classname: StochasticReconSubsetLoss
    coeff: 0.5
    routing:
      type: uniform_k_subset
  - classname: PGDReconSubsetLoss
    coeff: 0.5
    init: random
    step_size: 0.5
    n_steps: 4
    mask_scope: shared_across_batch
    routing:
      type: uniform_k_subset
  - classname: FaithfulnessLoss
    coeff: 10000.0
  - classname: ComponentWeightDecay
    coeff: 0.00
  - classname: ComponentL1Penalty
    coeff: 0.00
  - classname: DeadComponentWeightDecay
    coeff: 10000.0
    ci_threshold: 0.1
    patience: 200
output_loss_type: kl

# --- Training ---
batch_size: 8
steps: 120_000
lr_schedule:
  start_val: 3e-4
  fn_type: cosine
  warmup_pct: 0.01
  final_val_frac: 0.1
gradient_accumulation_steps: 1
grad_clip_norm_components: 1.0

# --- Faithfulness Warmup ---
faithfulness_warmup_steps: 200
faithfulness_warmup_lr: 0.01
faithfulness_warmup_weight_decay: 0.1

# --- Logging & Saving ---
train_log_freq: 100
eval_freq: 1000
slow_eval_freq: 5000
slow_eval_on_first_step: true
n_eval_steps: 5
save_freq: null
ci_alive_threshold: 0.0
eval_metric_configs:
  - classname: CIHistograms
    n_batches_accum: 5
  - classname: ComponentActivationDensity
  - classname: CI_L0
    groups: null
  - classname: CEandKLLosses
    rounding_threshold: 0.0
  - classname: CIMeanPerComponent
  - classname: StochasticHiddenActsReconLoss
  - classname: PGDReconSubsetLoss
    coeff: null
    init: random
    step_size: 1.0
    n_steps: 20
    mask_scope: shared_across_batch
    routing:
      type: uniform_k_subset

# --- Pretrained model ---
pretrained_model_class: transformers.GPT2LMHeadModel
pretrained_model_name: openai-community/gpt2
pretrained_model_output_attr: logits
tokenizer_name: openai-community/gpt2

# --- Data ---
task_config:
  task_name: lm
  max_seq_len: 256
  dataset_name: "Skylion007/openwebtext"
  is_tokenized: false
  column_name: "text"
  streaming: true
  train_data_split: "train"
  eval_data_split: "train"
